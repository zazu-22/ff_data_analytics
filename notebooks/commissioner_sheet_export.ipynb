{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commissioner Sheet Export to GCS\n",
    "\n",
    "Run this in Google Colab to export Commissioner Sheet data to GCS.\n",
    "Colab runs in Google's infrastructure and can often access sheets that timeout elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google (will prompt for auth)\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from google.auth import default\n",
    "from google.cloud import storage\n",
    "\n",
    "# Get authenticated credentials\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Commissioner Sheet\n",
    "SHEET_ID = \"1jYAGKzPmaQnmvomLzARw9mL6-JbguwkFQWlOfN7VGNY\"\n",
    "sheet = gc.open_by_key(SHEET_ID)\n",
    "print(f\"Opened: {sheet.title}\")\n",
    "print(f\"Worksheets: {[ws.title for ws in sheet.worksheets()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export only the specified owner tabs to DataFrames\n",
    "data = {}\n",
    "all_worksheets = {ws.title: ws for ws in sheet.worksheets()}\n",
    "print(f\"Total worksheets in sheet: {len(all_worksheets)}\")\n",
    "print(f\"Available worksheets: {list(all_worksheets.keys())}\\n\")\n",
    "\n",
    "for tab_name in OWNER_TABS:\n",
    "    if tab_name not in all_worksheets:\n",
    "        print(f\"⚠️  Tab '{tab_name}' not found - skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Reading {tab_name}...\", end=\" \")\n",
    "    ws = all_worksheets[tab_name]\n",
    "\n",
    "    try:\n",
    "        # Get all values from this owner's tab\n",
    "        values = ws.get_all_values()\n",
    "\n",
    "        if values and len(values) > 1:\n",
    "            # Convert to DataFrame (first row as headers)\n",
    "            df = pd.DataFrame(values[1:], columns=values[0])\n",
    "            # Clean up empty columns\n",
    "            df = df.loc[:, (df != \"\").any(axis=0)]\n",
    "            data[tab_name] = df\n",
    "            print(f\"✓ {len(df)} rows × {len(df.columns)} columns\")\n",
    "        else:\n",
    "            print(\"⚠️  No data found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Successfully exported {len(data)}/{len(OWNER_TABS)} owner tabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export each worksheet to a DataFrame\n",
    "data = {}\n",
    "for ws in sheet.worksheets():\n",
    "    print(f\"Reading {ws.title}...\")\n",
    "    try:\n",
    "        # Get all values\n",
    "        values = ws.get_all_values()\n",
    "        if values:\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(values[1:], columns=values[0])\n",
    "            data[ws.title] = df\n",
    "            print(f\"  ✓ {len(df)} rows × {len(df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GCS with organized structure\n",
    "\n",
    "bucket_name = \"ff-analytics\"\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Upload each owner's CSV to GCS\n",
    "dt = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "print(\"Uploading to GCS...\")\n",
    "for owner_name, df in data.items():\n",
    "    # Store by owner and date\n",
    "    blob_name = f\"raw/commissioner/rosters/{owner_name}/dt={dt}/data.csv\"\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Add metadata\n",
    "    blob.metadata = {\n",
    "        \"source\": \"commissioner_sheet\",\n",
    "        \"owner\": owner_name,\n",
    "        \"export_timestamp\": timestamp,\n",
    "        \"rows\": str(len(df)),\n",
    "        \"columns\": str(len(df.columns)),\n",
    "    }\n",
    "\n",
    "    # Upload from DataFrame\n",
    "    csv_data = df.to_csv(index=False)\n",
    "    blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "    print(f\"  ✓ {owner_name} → gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "print(f\"\\n✅ Upload complete! {len(data)} owner tabs exported to GCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of exported data\n",
    "print(\"=\" * 50)\n",
    "print(\"Export Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for owner_name, df in data.items():\n",
    "    print(f\"\\n{owner_name}:\")\n",
    "    print(f\"  Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "    # Show first few column names as a preview\n",
    "    cols = list(df.columns[:5])\n",
    "    if len(df.columns) > 5:\n",
    "        cols.append(\"...\")\n",
    "    print(f\"  Columns: {cols}\")\n",
    "\n",
    "    # Check for key columns (adjust based on your sheet structure)\n",
    "    key_cols = [\"Player\", \"Team\", \"Position\", \"Contract\"]\n",
    "    found_keys = [col for col in key_cols if col in df.columns]\n",
    "    if found_keys:\n",
    "        print(f\"  Key columns found: {found_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GCS\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = \"ff-analytics\"\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Upload each CSV to GCS\n",
    "dt = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "for sheet_name, df in data.items():\n",
    "    blob_name = f\"raw/commissioner/{sheet_name}/dt={dt}/data.csv\"\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Upload from DataFrame\n",
    "    blob.upload_from_string(df.to_csv(index=False), content_type=\"text/csv\")\n",
    "    print(f\"Uploaded to gs://{bucket_name}/{blob_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
